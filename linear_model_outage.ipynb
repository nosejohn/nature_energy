{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data matrix for fixed effects model...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "import gc \n",
    "\n",
    "df = pd.read_csv('/unequal_outage_data_redacted.csv') \n",
    "\n",
    "df_filtered = df[['anonymized_zip_id', 'date','anonymized_county', 'ln_customers_duration', 'heavy_ppt1','heat', 'cold', 'heavy_snow_fall', 'cyclone_binary', 'strong_wind', 'fire_binary',\n",
    "        'total_customers_served', 'state','substation_count', 'distance_to_nearest_substation', 'population_density', 'coastal', 'urban',\n",
    "         'tree_coverage_percentage', 'per_built_after_2010', 'per_built_1990_2009', 'per_built_1970_1989', 'per_built_1950_1969', 'ln_median_hhi', 'per_nonwhite',\n",
    "         'water_treatment', 'hospital', 'nursing_home' ,'per_old' ,'per_electricity'\n",
    "        ]]\n",
    "\n",
    "# Define target \n",
    "target_column = 'ln_customers_duration'\n",
    "\n",
    "# 2. FEATURE ENGINEERING\n",
    "# Define the weather and other variables to analyze\n",
    "weather_vars = ['cyclone_binary', 'strong_wind', 'fire_binary', 'heavy_ppt1', \n",
    "                'heat', 'cold', 'heavy_snow_fall']\n",
    "\n",
    "# Combine all predictors\n",
    "predictors = weather_vars\n",
    "\n",
    "# 3. MISSING VALUES\n",
    "df_clean = df_filtered.copy()\n",
    "\n",
    "# Drop rows with missing values in our key variables\n",
    "missing_before = len(df_clean)\n",
    "df_clean = df_clean.dropna(subset=[target_column] + predictors)\n",
    "missing_after = len(df_clean)\n",
    "\n",
    "# Create anonymized_zip_id and date dummies for fixed effects\n",
    "df_clean['anonymized_zip_id'] = df_clean['anonymized_zip_id'].astype(str)\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date']).dt.date.astype(str)\n",
    "\n",
    "# 5. IMPLEMENTING PANEL REGRESSION WITH FIXED EFFECTS\n",
    "# Build formula string for regression with fixed effects\n",
    "formula_parts = [target_column, '~'] + predictors + ['C(anonymized_zip_id)', 'C(date)']\n",
    "formula = ' + '.join(formula_parts)\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Preparing data matrix for fixed effects model...\")\n",
    "    y = df_clean[target_column]\n",
    "    \n",
    "    # Remove sparse parameter as it's not supported in your patsy version\n",
    "    X = patsy.dmatrix(formula.split('~')[1], data=df_clean, return_type='dataframe')\n",
    "    \n",
    "    # Add constant\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Run regression with fixed effects\n",
    "    print(\"Running fixed effects regression...\")\n",
    "    model = sm.OLS(y, X).fit(cov_type='cluster', cov_kwds={'groups': df_clean['anonymized_zip_id']})\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nFixed Effects Regression Results:\")\n",
    "    print(model.summary())\n",
    "    \n",
    "    # Save the regression results to a text file\n",
    "    with open('fixed_effects_results.txt', 'w') as f:\n",
    "        f.write(model.summary().as_text())\n",
    "    \n",
    "    print(\"1. fixed_effects_results.txt - Fixed effects regression results\")\n",
    "    \n",
    "    # Free up memory\n",
    "    del X\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error in fixed effects model: {str(e)}\")\n",
    "    print(\"Skipping main fixed effects model and proceeding with anonymized_zip_id-specific analysis...\")\n",
    "\n",
    "# 6. anonymized_zip_id-SPECIFIC ANALYSIS WITHOUT INTERACTION MODEL\n",
    "print(\"\\nAnalyzing anonymized_zip_id-specific weather effects (no interaction model)...\")\n",
    "\n",
    "# Reduce memory usage by using only necessary columns\n",
    "essential_columns = [target_column, 'anonymized_zip_id', 'date'] + predictors\n",
    "df_minimal = df_clean[essential_columns].copy()\n",
    "\n",
    "# Alternative approach: Run separate regressions for each anonymized_zip_id\n",
    "all_anonymized_zip_ids = df_minimal['anonymized_zip_id'].unique().tolist()\n",
    "print(f\"Total anonymized_zip_ids to analyze: {len(all_anonymized_zip_ids)}\")\n",
    "\n",
    "# Prepare a dataframe to store coefficients for each anonymized_zip_id\n",
    "anonymized_zip_id_coefficients = pd.DataFrame(columns=['anonymized_zip_id'] + weather_vars)\n",
    "\n",
    "# Process anonymized_zip_ids in batches to manage memory\n",
    "BATCH_SIZE = 50\n",
    "num_batches = (len(all_anonymized_zip_ids) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "for batch_idx in range(num_batches):\n",
    "    batch_start = batch_idx * BATCH_SIZE\n",
    "    batch_end = min((batch_idx + 1) * BATCH_SIZE, len(all_anonymized_zip_ids))\n",
    "    batch_anonymized_zip_ids = all_anonymized_zip_ids[batch_start:batch_end]\n",
    "    \n",
    "    print(f\"\\nProcessing batch {batch_idx+1}/{num_batches} with {len(batch_anonymized_zip_ids)} anonymized_zip_ids...\")\n",
    "    \n",
    "    # Process each anonymized_zip_id in the batch\n",
    "    for anonymized_zip_id in tqdm(batch_anonymized_zip_ids):\n",
    "        # Filter data for this anonymized_zip_id\n",
    "        anonymized_zip_id_data = df_minimal[df_minimal['anonymized_zip_id'] == anonymized_zip_id].copy()\n",
    "        \n",
    "        # Skip if too few observations\n",
    "        if len(anonymized_zip_id_data) < 30:  # Minimum observations for meaningful regression\n",
    "            print(f\"Skipping anonymized_zip_id {anonymized_zip_id}: insufficient observations ({len(anonymized_zip_id_data)})\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Create regression formula for this anonymized_zip_id\n",
    "            # We don't need anonymized_zip_id fixed effects when analyzing a single anonymized_zip_id\n",
    "            anonymized_zip_id_formula_parts = [target_column, '~'] + predictors + ['C(date)']\n",
    "            anonymized_zip_id_formula = ' + '.join(anonymized_zip_id_formula_parts)\n",
    "            \n",
    "            # Prepare data\n",
    "            y_zip = anonymized_zip_id_data[target_column]\n",
    "            X_zip = patsy.dmatrix(anonymized_zip_id_formula.split('~')[1], data=anonymized_zip_id_data, return_type='dataframe')\n",
    "            X_zip = sm.add_constant(X_zip)\n",
    "            \n",
    "            # Run regression for this anonymized_zip_id\n",
    "            anonymized_zip_id_model = sm.OLS(y_zip, X_zip).fit()\n",
    "            \n",
    "            # Extract weather coefficients\n",
    "            coef_row = {'anonymized_zip_id': anonymized_zip_id}\n",
    "            for var in weather_vars:\n",
    "                # Get coefficient if it exists in the model\n",
    "                try:\n",
    "                    coef_row[var] = anonymized_zip_id_model.params[var]\n",
    "                except:\n",
    "                    # The variable might have a prefix in the model\n",
    "                    try:\n",
    "                        param_name = [name for name in anonymized_zip_id_model.params.index if name.endswith(var)]\n",
    "                        if param_name:\n",
    "                            coef_row[var] = anonymized_zip_id_model.params[param_name[0]]\n",
    "                        else:\n",
    "                            coef_row[var] = np.nan\n",
    "                    except:\n",
    "                        coef_row[var] = np.nan\n",
    "                    \n",
    "            # Add to results dataframe\n",
    "            anonymized_zip_id_coefficients = pd.concat([anonymized_zip_id_coefficients, \n",
    "                                             pd.DataFrame([coef_row])], \n",
    "                                             ignore_index=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing anonymized_zip_id {anonymized_zip_id}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "        # Clean up to save memory\n",
    "        del anonymized_zip_id_data, X_zip, anonymized_zip_id_model\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Completed batch {batch_idx+1}/{num_batches}\")\n",
    "\n",
    "# 7. PROCESS AND VISUALIZE THE RESULTS\n",
    "print(\"\\nProcessing results and creating visualizations...\")\n",
    "\n",
    "if len(anonymized_zip_id_coefficients) > 0:\n",
    "    # Save the full coefficients data to CSV\n",
    "    print(\"Saving anonymized_zip_id coefficients to CSV...\")\n",
    "    anonymized_zip_id_coefficients.to_csv('anonymized_zip_id_weather_coefficients.csv', index=False)\n",
    "    \n",
    "    # Set anonymized_zip_id as index for heatmap\n",
    "    heatmap_data = anonymized_zip_id_coefficients.set_index('anonymized_zip_id')\n",
    "    \n",
    "    # Create a cleaner version for analysis\n",
    "    vulnerability_scores = heatmap_data.copy()\n",
    "    # For vulnerability, we're interested in negative coefficients (more disruption)\n",
    "    vulnerability_scores['avg_vulnerability'] = -vulnerability_scores[weather_vars].mean(axis=1)\n",
    "    vulnerability_scores = vulnerability_scores.sort_values('avg_vulnerability', ascending=False)\n",
    "    \n",
    "    # Save this sorted version\n",
    "    vulnerability_scores.to_csv('anonymized_zip_id_vulnerability_scores.csv')\n",
    "    \n",
    "    # For the heatmap, limit to top 30 anonymized_zip_ids by vulnerability\n",
    "    top_vulnerable_anonymized_zip_ids = vulnerability_scores.head(30).index.tolist()\n",
    "    if len(top_vulnerable_anonymized_zip_ids) > 0:\n",
    "        top_heatmap_data = heatmap_data.loc[top_vulnerable_anonymized_zip_ids, weather_vars]\n",
    "        \n",
    "        # Plot heatmap for top 30 most vulnerable anonymized_zip_ids\n",
    "        plt.figure(figsize=(16, 12))\n",
    "        sns.heatmap(top_heatmap_data, cmap='RdBu_r', center=0, annot=True, fmt='.3f')\n",
    "        plt.title('Top 30 Most Vulnerable anonymized_zip_ids to Weather Events')\n",
    "        plt.xlabel('Weather Variables')\n",
    "        plt.ylabel('anonymized_zip_ids')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('top30_anonymized_zip_id_weather_vulnerability.png')\n",
    "    \n",
    "    # Create a full heatmap only if it's a reasonable size\n",
    "    if len(anonymized_zip_id_coefficients) < 200:  # Limit full heatmap to manageable size\n",
    "        plt.figure(figsize=(20, 24))\n",
    "        heatmap_all = heatmap_data[weather_vars]\n",
    "        sns.heatmap(heatmap_all, cmap='RdBu_r', center=0, annot=False)\n",
    "        plt.title('All anonymized_zip_id Vulnerability to Weather Events')\n",
    "        plt.xlabel('Weather Variables')\n",
    "        plt.ylabel('anonymized_zip_ids')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('all_anonymized_zip_id_weather_vulnerability.png')\n",
    "    else:\n",
    "        print(f\"Skipping full heatmap generation due to large number of anonymized_zip_ids ({len(anonymized_zip_id_coefficients)})\")\n",
    "    \n",
    "    # Create summary statistics for each weather variable\n",
    "    print(\"\\nGenerating summary statistics on anonymized_zip_id vulnerability by weather type...\")\n",
    "    weather_vulnerability_summary = pd.DataFrame({\n",
    "        'weather_type': weather_vars,\n",
    "        'mean_effect': [heatmap_data[var].mean() for var in weather_vars],\n",
    "        'median_effect': [heatmap_data[var].median() for var in weather_vars],\n",
    "        'std_dev': [heatmap_data[var].std() for var in weather_vars],\n",
    "        'min_effect': [heatmap_data[var].min() for var in weather_vars],\n",
    "        'max_effect': [heatmap_data[var].max() for var in weather_vars],\n",
    "        'most_vulnerable_anonymized_zip_id': [heatmap_data[var].idxmin() for var in weather_vars],\n",
    "        'least_vulnerable_anonymized_zip_id': [heatmap_data[var].idxmax() for var in weather_vars]\n",
    "    })\n",
    "    \n",
    "    # Save the summary statistics\n",
    "    weather_vulnerability_summary.to_csv('weather_vulnerability_summary.csv', index=False)\n",
    "    \n",
    "    if len(anonymized_zip_id_coefficients) < 200:\n",
    "        print(\"5. all_anonymized_zip_id_weather_vulnerability.png - Full heatmap of all anonymized_zip_id vulnerabilities\")\n",
    "    \n",
    "    if 'model' in locals():\n",
    "        print(\"6. fixed_effects_results.txt - Main fixed effects regression results\")\n",
    "else:\n",
    "    print(\"Warning: No anonymized_zip_id coefficients were successfully extracted. Check for errors in the batch processing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
